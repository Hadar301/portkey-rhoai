
================================================================================
   Portkey AI Gateway has been deployed!
================================================================================

Release: {{ .Release.Name }}
Namespace: {{ .Release.Namespace }}

--------------------------------------------------------------------------------
  ACCESS THE GATEWAY
--------------------------------------------------------------------------------

{{- if .Values.route.enabled }}

Your gateway is accessible via OpenShift Route:

{{- if .Values.route.host }}
  URL: https://{{ .Values.route.host }}{{ .Values.route.path }}
{{- else }}

  Get the route URL:
  $ oc get route {{ include "portkey-gateway.fullname" . }} -n {{ .Release.Namespace }} -o jsonpath='{.spec.host}'
{{- end }}

{{- else }}

The gateway is only accessible within the cluster via:
  Service: {{ include "portkey-gateway.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.service.port }}

To expose externally, enable the route in values.yaml:
  route:
    enabled: true

{{- end }}

--------------------------------------------------------------------------------
  VERIFY DEPLOYMENT
--------------------------------------------------------------------------------

Check pod status:
  $ oc get pods -n {{ .Release.Namespace }} -l app.kubernetes.io/name={{ include "portkey-gateway.name" . }}

Check logs:
  $ oc logs -n {{ .Release.Namespace }} -l app.kubernetes.io/name={{ include "portkey-gateway.name" . }} -f

Health check:
  $ curl -s https://$(oc get route {{ include "portkey-gateway.fullname" . }} -n {{ .Release.Namespace }} -o jsonpath='{.spec.host}')/health

--------------------------------------------------------------------------------
  USING THE GATEWAY
--------------------------------------------------------------------------------

The Portkey Gateway acts as a proxy to various LLM providers. Example usage:

  curl -X POST "https://<gateway-url>/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "x-portkey-provider: openai" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
      "model": "gpt-4",
      "messages": [{"role": "user", "content": "Hello!"}]
    }'

For more information, visit: https://portkey.ai/docs

{{- if .Values.redis.enabled }}

--------------------------------------------------------------------------------
  REDIS CACHE
--------------------------------------------------------------------------------

Redis is deployed for caching. Connection details:
  Host: {{ .Release.Name }}-redis-master
  Port: 6379
{{- if .Values.redis.auth.enabled }}
  Password: Stored in secret "{{ .Release.Name }}-redis"
{{- end }}

{{- end }}

{{- if .Values.ollama.enabled }}

--------------------------------------------------------------------------------
  OLLAMA (LOCAL LLM)
--------------------------------------------------------------------------------

Ollama is deployed for local LLM inference.
  Internal URL: http://{{ include "portkey-gateway.fullname" . }}-ollama:11434
  Model: {{ .Values.ollama.model }}

To use Ollama via the Portkey Gateway:

  GATEWAY_URL=$(oc get route {{ include "portkey-gateway.fullname" . }} -n {{ .Release.Namespace }} -o jsonpath='{.spec.host}')
  
  curl -X POST "https://${GATEWAY_URL}/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "x-portkey-provider: ollama" \
    -H "x-portkey-custom-host: http://{{ include "portkey-gateway.fullname" . }}-ollama:11434" \
    -d '{
      "model": "{{ .Values.ollama.model }}",
      "messages": [{"role": "user", "content": "Hello!"}]
    }'

Check Ollama status:
  $ oc get pods -n {{ .Release.Namespace }} -l app.kubernetes.io/component=ollama

Check if model is downloaded:
  $ oc exec -n {{ .Release.Namespace }} $(oc get pod -n {{ .Release.Namespace }} -l app.kubernetes.io/component=ollama -o jsonpath='{.items[0].metadata.name}') -- ollama list

{{- end }}

================================================================================

